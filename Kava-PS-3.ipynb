{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3\n",
    "In this homework you'll work with NLTK using the NYT comments data you collected in Tutorial 2. You can download that [here](https://github.com/comp-journalism/UMD-J479V-J779V-Spring2017/blob/master/Data/NYT-Comments-12-15-16.csv?raw=true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What are the 25 most frequent unigrams (i.e. words) used across all the comments?\n",
    "*Hint*: Follow and adapt the basic steps outlined in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paulymath\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Load the csv in the dataframe\n",
    "file_df = pd.read_csv('NYT-Comments-12-15-16.csv')\n",
    "#Convert all the comment bodies to form 1 string\n",
    "print file_df.userDisplayName[0]\n",
    "filedf_string=', '.join(file_df.commentBody)\n",
    "filedf_string = unicode(filedf_string, \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert the string to lower case\n",
    "filedf_lower = filedf_string.lower() \n",
    "\n",
    "table = {\n",
    "    ord(u'\\u2018') : u\"'\",\n",
    "    ord(u'\\u2019') : u\"'\",\n",
    "    ord(u'\\u201C') : u'\"',\n",
    "    ord(u'\\u201d') : u'\"',\n",
    "    ord(u'\\u2026') : u'', \n",
    "    ord(u'\\u2014') : u'',\n",
    "}\n",
    "#Convert the punctuation marks to standard format\n",
    "filedf_lower = filedf_lower.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    #Grab the list of standard punctuation symbols that are provided in the string library\n",
    "    punctuations = string.punctuation # includes following characters: !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\n",
    "\n",
    "    #Don't strip out apostrophes\n",
    "    excluded_punctuations = [\"'\"]\n",
    "    for p in punctuations:\n",
    "        if p not in excluded_punctuations:\n",
    "            #Replace each punctuation symbol by a space\n",
    "            text = text.replace(p, \" \") \n",
    "    return text\n",
    "\n",
    "#Remove unnecessary punctuation\n",
    "filedf_lower_wopun = remove_punctuation(filedf_lower)\n",
    "\n",
    "#Remove multiple spaces\n",
    "filedf_lower_wopun=\" \".join(filedf_lower_wopun.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "#Create a tokenizer from NLTK which will create tokens based on the whitespace in between words\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "#Use the tokenizer on the text string\n",
    "file_tokens = tokenizer.tokenize(filedf_lower_wopun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use the stopwords in the NLTK library\n",
    "from nltk.corpus import stopwords\n",
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "#Eliminate stopwords from the string\n",
    "transcript_tokens = [w for w in file_tokens if w not in stopword_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'br', 12455), (u'trump', 4005), (u'would', 1951), (u'people', 1900), (u'one', 1611), (u'us', 1419), (u'like', 1415), (u'vote', 1214), (u\"it's\", 1053), (u'election', 1043), (u'president', 1035), (u\"don't\", 1030), (u'get', 1015), (u'even', 945), (u'time', 926), (u'right', 900), (u'think', 891), (u'many', 876), (u'country', 866), (u'electoral', 840), (u'years', 816), (u'mr', 798), (u'states', 792), (u'state', 789), (u'way', 778)]\n"
     ]
    }
   ],
   "source": [
    "#Display the most common 25 unigrams in the strong\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Construct the distribution (it will count the number of occurrances of each unique token)\n",
    "frequency_distribution = FreqDist(transcript_tokens)\n",
    "print frequency_distribution.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'trump', 4005),\n",
       " (u'would', 1951),\n",
       " (u'people', 1900),\n",
       " (u'one', 1611),\n",
       " (u'us', 1419),\n",
       " (u'like', 1415),\n",
       " (u'vote', 1214),\n",
       " (u\"it's\", 1053),\n",
       " (u'election', 1043),\n",
       " (u'president', 1035),\n",
       " (u\"don't\", 1030),\n",
       " (u'get', 1015),\n",
       " (u'even', 945),\n",
       " (u'time', 926),\n",
       " (u'right', 900),\n",
       " (u'think', 891),\n",
       " (u'many', 876),\n",
       " (u'country', 866),\n",
       " (u'electoral', 840),\n",
       " (u'years', 816),\n",
       " (u'mr', 798),\n",
       " (u'states', 792),\n",
       " (u'state', 789),\n",
       " (u'way', 778),\n",
       " (u'republicans', 778)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove unwanted words from the list\n",
    "del frequency_distribution[\"br\"]\n",
    "frequency_distribution.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. According to the first name of the user who made each comment, how many women vs. men made comments?\n",
    "To determine the gender of the name, first split the `userDisplayName` field so you isolate the first name. You may have to drop any rows which do not have a userDisplayName. Then use the [Sex Machine library](https://github.com/ferhatelmas/sexmachine/) to tag the sex of the name. Be sure to read and understand the Sex Machine documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://pypi.python.org/packages/dd/01/cc5b32af2b3658079736bd865019aeb8db04f9c5764eac72185c276f3aac/SexMachine-0.1.1.tar.gz\n",
      "  Using cached SexMachine-0.1.1.tar.gz\n",
      "  Requirement already satisfied (use --upgrade to upgrade): SexMachine==0.1.1 from https://pypi.python.org/packages/dd/01/cc5b32af2b3658079736bd865019aeb8db04f9c5764eac72185c276f3aac/SexMachine-0.1.1.tar.gz in c:\\users\\kavas\\anaconda2\\lib\\site-packages\n",
      "Building wheels for collected packages: SexMachine\n",
      "  Running setup.py bdist_wheel for SexMachine: started\n",
      "  Running setup.py bdist_wheel for SexMachine: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\kavas\\AppData\\Local\\pip\\Cache\\wheels\\0a\\9a\\19\\ec2dd809aeda18aff90f4cf5d9dd1da4b1187fdc7a5948061c\n",
      "Successfully built SexMachine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#Install the Sex Machine library\n",
    "! pip install https://pypi.python.org/packages/dd/01/cc5b32af2b3658079736bd865019aeb8db04f9c5764eac72185c276f3aac/SexMachine-0.1.1.tar.gz\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extracting the first name from the userDisplayName\n",
    "file_df[\"first_name\"]=file_df.userDisplayName.str.extract('([A-z]\\w{0,})', expand=True)\n",
    "file_df=file_df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3427\n",
      "Gender\n",
      "Unknown          1902\n",
      "female            455\n",
      "male              937\n",
      "mostly_female      64\n",
      "mostly_male        69\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import sexmachine.detector as gender\n",
    "\n",
    "#Set value as unknown for names not in the list\n",
    "d = gender.Detector(unknown_value=u\"Unknown\")\n",
    "gender_df=[]\n",
    "\n",
    "#Identify gender based on first name for individuals\n",
    "print len(file_df)\n",
    "for i in range(len(file_df)):\n",
    "    gender=d.get_gender(file_df.first_name[i])\n",
    "    gender_df.append(gender)\n",
    "\n",
    "#Copy the list to the dataframe\n",
    "file_df[\"Gender\"]=gender_df\n",
    "\n",
    "#Group by gender and display the frequency\n",
    "print file_df.groupby(\"Gender\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What are the average positivity and negativity scores of all comments?\n",
    "*Hint*: Follow and adapt the basic steps outlined in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extract positive and negative terms from the text file\n",
    "positive_terms = []\n",
    "f = open('positive_terms.txt', \"r\")\n",
    "positive_terms = f.read().splitlines()\n",
    "f.close()\n",
    "\n",
    "negative_terms = []\n",
    "f = open('negative_terms.txt', \"r\")\n",
    "negative_terms = f.read().splitlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function will take a string and lowercase it, remove punctuations and spaces, and then tokentize and stem those tokens\n",
    "import nltk\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "file_df = pd.read_csv('NYT-Comments-12-15-16.csv',encoding='utf-8')\n",
    "def normalize_review_text(text):\n",
    "    text = text.lower()\n",
    "    text = remove_punctuation(text)\n",
    "    text = \" \".join(text.split())\n",
    "    text_tokens = tokenizer.tokenize(text)\n",
    "    text_tokens = [porter.stem(w) for w in text_tokens if w not in stopword_list]\n",
    "    return text_tokens\n",
    "\n",
    "# Apply the function above to the text column\n",
    "file_df[\"commentBody\"] = file_df[\"commentBody\"].apply(normalize_review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate the positivity and negativity score for comments\n",
    "def calculate_positivity(text):\n",
    "    num_tokens = len(text)\n",
    "    if num_tokens==0:\n",
    "        return 0\n",
    "    num_positive_tokens = 0\n",
    "    for t in text:\n",
    "        if t in positive_terms:\n",
    "            num_positive_tokens = num_positive_tokens + 1\n",
    "    # The positivity score is the fraction of tokens that were found in the positive dictionary\n",
    "    return float(num_positive_tokens) / float(num_tokens)\n",
    "\n",
    "file_df[\"positivity_score\"] = file_df[\"commentBody\"].apply(calculate_positivity)\n",
    "\n",
    "def calculate_negativity(text):\n",
    "    num_tokens = len(text)\n",
    "    if num_tokens==0:\n",
    "        return 0\n",
    "    num_negative_tokens = 0\n",
    "    for t in text:\n",
    "        if t in negative_terms:\n",
    "            num_negative_tokens = num_negative_tokens + 1\n",
    "    # The positivity score is the fraction of tokens that were found in the positive dictionary\n",
    "    return float(num_negative_tokens) / float(num_tokens)\n",
    "\n",
    "file_df[\"negativity_score\"] = file_df[\"commentBody\"].apply(calculate_negativity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Positivity Score: 0.0552719783149\n",
      "Average Negativity Score: 0.0342588723472\n"
     ]
    }
   ],
   "source": [
    "#Print the average value of the positivity and negativity score\n",
    "pos_score=file_df[\"positivity_score\"].mean()\n",
    "neg_score=file_df[\"negativity_score\"].mean()\n",
    "\n",
    "print \"Average Positivity Score:\",pos_score\n",
    "print \"Average Negativity Score:\",neg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What are the average positivity and negativity scores for comments marked as editor's selections compared to those not marked as editor's selections?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Positivity Score for comments marked as editor's selections: 0.0508764913456\n",
      "Average Negativity Score for comments marked as editor's selections: 0.0399209228364\n",
      "Average Positivity Score for comments not marked as editor's selections: 0.0553566125976\n",
      "Average Negativity Score for comments not marked as editor's selections: 0.0341498506199\n"
     ]
    }
   ],
   "source": [
    "#Split the dataset into 2 based on editors selection\n",
    "mask_bool_editor=(file_df[\"editorsSelection\"] == 1)\n",
    "\n",
    "mask_bool_noteditor=(file_df[\"editorsSelection\"] == 0)\n",
    "\n",
    "#Calculate the positivity and negativity score for comments\n",
    "editor_pos_df=file_df.loc[mask_bool_editor][\"commentBody\"].apply(calculate_positivity)\n",
    "editor_neg_df=file_df.loc[mask_bool_editor][\"commentBody\"].apply(calculate_negativity)\n",
    "noteditor_pos_df=file_df.loc[mask_bool_noteditor][\"commentBody\"].apply(calculate_positivity)\n",
    "noteditor_neg_df=file_df.loc[mask_bool_noteditor][\"commentBody\"].apply(calculate_negativity)\n",
    "\n",
    "#Print the average value of the positivity and negativity score\n",
    "editor_pos = editor_pos_df.mean()\n",
    "editor_neg = editor_neg_df.mean()\n",
    "noteditor_pos = noteditor_pos_df.mean()\n",
    "noteditor_neg = noteditor_neg_df.mean()\n",
    "\n",
    "print \"Average Positivity Score for comments marked as editor's selections:\",editor_pos\n",
    "print \"Average Negativity Score for comments marked as editor's selections:\",editor_neg\n",
    "print \"Average Positivity Score for comments not marked as editor's selections:\",noteditor_pos\n",
    "print \"Average Negativity Score for comments not marked as editor's selections:\",noteditor_neg"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
